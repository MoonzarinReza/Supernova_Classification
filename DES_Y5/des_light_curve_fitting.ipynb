{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c65ff99",
   "metadata": {},
   "source": [
    "# DES light curves fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ee0a6",
   "metadata": {},
   "source": [
    "### 1. Imports \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab80068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "from numpy import log10 as log10\n",
    "from astropy.table import Table\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import parsnip\n",
    "import lcdata\n",
    "import sncosmo\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import *\n",
    "import scipy.optimize as opt\n",
    "from chainconsumer import ChainConsumer\n",
    "import emcee\n",
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "from pandas import DataFrame\n",
    "import scipy.stats as st\n",
    "import corner\n",
    "from mpfit import mpfit\n",
    "#import mpfit\n",
    "from scipy.interpolate import interp1d\n",
    "import math\n",
    "from numpy import sqrt as sqrt\n",
    "from numpy import square as square\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38c8b5",
   "metadata": {},
   "source": [
    "### 2. Data Configuration and Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a28b9a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of photometry datapoints: 1798736\n",
      "Number of transients: 19706\n"
     ]
    }
   ],
   "source": [
    "tr=3\n",
    "t1=4\n",
    "countshow=0\n",
    "\n",
    "\n",
    "color=['cyan','green', 'red',  'blue', 'm', 'black']\n",
    "dict_color={ 'g':color[1], 'r':color[2], 'i':color[3], 'z':color[4]}\n",
    "dict_number={ 'g':3.0, 'r':1.0, 'i':-1.0, 'z':-3.0}\n",
    "dict_sym={'g':'s', 'r':'*', 'i':'o', 'z':'^'}\n",
    "dict_legend={ 'g':'g+3.0', 'r':'r+1.0', 'i':'i-1.0', 'z':'z-3.0'}\n",
    "dict_ms={'g':40, 'r':75, 'i':45, 'z':60}\n",
    "\n",
    "data_3=[]\n",
    "table_attm3=[]\n",
    "\n",
    "\n",
    "\n",
    "phot_df = Table.read('lcs.FITS').to_pandas()\n",
    "head_df = Table.read('meta.FITS').to_pandas()\n",
    "mapping_coeff=pd.read_csv('mapping_coeff_original.csv')\n",
    "mapping_coeff = mapping_coeff.set_index('Filter')\n",
    "\n",
    "print(\"Number of photometry datapoints:\", len(phot_df))\n",
    "print(\"Number of transients:\", len(head_df))\n",
    "\n",
    "eigen_val=np.genfromtxt('eigen_val.txt')\n",
    "\n",
    "ph=eigen_val[1:,0]\n",
    "vec0=eigen_val[1:,1]\n",
    "vec1=eigen_val[1:,2]\n",
    "vec2=eigen_val[1:,3]\n",
    "\n",
    "\n",
    "imod0 = interp1d(ph, vec0, fill_value='extrapolate')\n",
    "imod1 = interp1d(ph, vec1, fill_value='extrapolate')\n",
    "imod2 = interp1d(ph, vec2, fill_value='extrapolate')\n",
    "\n",
    "os.makedirs('tables', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37b5d4",
   "metadata": {},
   "source": [
    "### 4. Model and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b85218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "def leng_calc1(time_array2, t_o2):\n",
    "    mphase = (time_array2-t_o2)/(1+redshift_p)\n",
    "    indsel=(np.where((mphase>=-10.0) & (mphase<=40.0)))[0]\n",
    "    return len(indsel)\n",
    "\n",
    "\n",
    "def modelcurve1(x_calc, theta, arr_pass):\n",
    "    t_min, mag_o, a1, a2 = theta\n",
    "    mphase = (x_calc-t_min)/(1+redshift_p)\n",
    "\n",
    "    if mphase >= -10.0 and mphase <= 40.0:\n",
    "        VecP0 = float(imod0(mphase))\n",
    "        VecP1 = float(imod1(mphase))\n",
    "        VecP2 = float(imod2(mphase))\n",
    "\n",
    "        y_calc = mag_o + VecP0 + a1*VecP1 + a2*VecP2\n",
    "       \n",
    "        err_pc_scores=np.square(arr_pass[0]* VecP1)+np.square(arr_pass[1]*VecP2)\n",
    "      \n",
    "        \n",
    "        err_cal= err_pc_scores\n",
    "      \n",
    "    else:\n",
    "        y_calc=np.nan\n",
    "        err_cal=np.nan\n",
    "          \n",
    "    return y_calc, err_cal\n",
    "\n",
    "\n",
    "\n",
    "def myfunctlin1(p, fjac=None, x=None, y=None, err=None, arr_pass=None, ini_params=None, priors=None):\n",
    "    \n",
    "    dev = np.zeros((len(x)), dtype = np.float64)\n",
    "    pr_arr = np.zeros((2), dtype = np.float64)\n",
    "   \n",
    "    \n",
    "    for kk in range(len(dev)):\n",
    "        \n",
    "        y_calc, tem=modelcurve1(x[kk], p, arr_pass)\n",
    "        \n",
    "        if not np.isnan(y_calc):\n",
    "            dev[kk]=(y[kk]-y_calc)/sqrt(np.square(err[kk])+ tem)\n",
    "    \n",
    "    pr_arr[0]=(p[2]-priors[0])/(sqrt(2)*arr_pass[0])\n",
    "    pr_arr[1]=(p[3]-priors[1])/(sqrt(2)*arr_pass[1])\n",
    "\n",
    "\n",
    "    dev_f=np.concatenate((dev, pr_arr))   \n",
    "    #print(len(dev_f))    \n",
    "    status = 0\n",
    "    # print('dev')\n",
    "    # print(len(dev_f))\n",
    "    return [status, dev_f]\n",
    "\n",
    "\n",
    "def mpfit_run_flux1(time_array, data_vec, data_vec_unc, ini_params, tym_cons_low, tym_cons_high, arr_pass, priors, redshift_p):\n",
    "\n",
    "    leng=leng_calc1(time_array, ini_params[0]) \n",
    "    #print('leng')\n",
    "    #print(leng)\n",
    "    if(leng<4):\n",
    "        \n",
    "        return np.zeros((4)), np.zeros((4)), 10000, 0\n",
    "    p0=ini_params\n",
    "   \n",
    "    priors=priors\n",
    "    parbase={'value':0., 'fixed':0, 'limited':[0,0], 'limits':[0.,0.]}\n",
    "    parinfo=[]\n",
    "    for i in range(len(p0)):\n",
    "        parinfo.append({'value':0., 'fixed':0, 'limited':[0,0], 'limits':[0.,0.]})\n",
    "    for i in range(len(p0)): \n",
    "        parinfo[i]['value']=p0[i]\n",
    "\n",
    "    parinfo[0]['limited'][0] = 1\n",
    "    parinfo[0]['limits'][0]  = tym_cons_low\n",
    "    parinfo[0]['limited'][1] = 1\n",
    "    parinfo[0]['limits'][1]  = tym_cons_high\n",
    "\n",
    "\n",
    "    parinfo[2]['limited'][0] = 1\n",
    "    parinfo[2]['limits'][0]  = -10\n",
    "    parinfo[2]['limited'][1] = 1\n",
    "    parinfo[2]['limits'][1]  = 10\n",
    "\n",
    "    parinfo[3]['limited'][0] = 1\n",
    "    parinfo[3]['limits'][0]  = -10\n",
    "    parinfo[3]['limited'][1] = 1\n",
    "    parinfo[3]['limits'][1]  = 10\n",
    "\n",
    "\n",
    "    fa = {'x':time_array, 'y':data_vec, 'err':data_vec_unc, 'arr_pass': arr_pass, 'ini_params':ini_params, 'priors': priors}\n",
    "    \n",
    "    m = mpfit(myfunctlin1,  parinfo=parinfo, quiet=True, functkw=fa)\n",
    "\n",
    "    if (m.status <= 0): \n",
    "        #print( 'error message = ', m.errmsg)\n",
    "        am=0.0\n",
    "    else:\n",
    "        am=m.fnorm\n",
    "\n",
    "    #print(m.params, m.perror, am)\n",
    "    return m.params, m.perror, am, leng-2\n",
    "\n",
    "\n",
    "print('yes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b4857",
   "metadata": {},
   "source": [
    "### Main loop over light curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6afb718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ii=0\n",
      "ii=100\n",
      "ii=200\n",
      "ii=300\n",
      "ii=400\n",
      "ii=500\n",
      "ii=600\n",
      "ii=700\n",
      "ii=800\n",
      "ii=900\n",
      "ii=1000\n",
      "ii=1100\n",
      "ii=1200\n",
      "ii=1300\n",
      "ii=1400\n",
      "ii=1500\n",
      "ii=1600\n",
      "ii=1700\n",
      "ii=1800\n",
      "ii=1900\n",
      "ii=2000\n",
      "ii=2100\n",
      "ii=2200\n",
      "ii=2300\n",
      "ii=2400\n",
      "ii=2500\n",
      "ii=2600\n",
      "ii=2700\n",
      "ii=2800\n",
      "ii=2900\n",
      "ii=3000\n",
      "ii=3100\n",
      "ii=3200\n",
      "ii=3300\n",
      "ii=3400\n",
      "ii=3500\n",
      "ii=3600\n",
      "ii=3700\n",
      "ii=3800\n",
      "ii=3900\n",
      "ii=4000\n",
      "ii=4100\n",
      "ii=4200\n",
      "ii=4300\n",
      "ii=4400\n",
      "ii=4500\n",
      "ii=4600\n",
      "ii=4700\n",
      "ii=4800\n",
      "ii=4900\n",
      "ii=5000\n",
      "ii=5100\n",
      "ii=5200\n",
      "ii=5300\n",
      "ii=5400\n",
      "ii=5500\n",
      "ii=5600\n",
      "ii=5700\n",
      "ii=5800\n",
      "ii=5900\n",
      "ii=6000\n",
      "ii=6100\n",
      "ii=6200\n",
      "ii=6300\n",
      "ii=6400\n",
      "ii=6500\n",
      "ii=6600\n",
      "ii=6700\n",
      "ii=6800\n",
      "ii=6900\n",
      "ii=7000\n",
      "ii=7100\n",
      "ii=7200\n",
      "ii=7300\n",
      "ii=7400\n",
      "ii=7500\n",
      "ii=7600\n",
      "ii=7700\n",
      "ii=7800\n",
      "ii=7900\n",
      "ii=8000\n",
      "ii=8100\n",
      "ii=8200\n",
      "ii=8300\n",
      "ii=8400\n",
      "ii=8500\n",
      "ii=8600\n",
      "ii=8700\n",
      "ii=8800\n",
      "ii=8900\n",
      "ii=9000\n",
      "ii=9100\n",
      "ii=9200\n",
      "ii=9300\n",
      "ii=9400\n",
      "ii=9500\n",
      "ii=9600\n",
      "ii=9700\n",
      "ii=9800\n",
      "ii=9900\n",
      "ii=10000\n",
      "ii=10100\n",
      "ii=10200\n",
      "ii=10300\n",
      "ii=10400\n",
      "ii=10500\n",
      "ii=10600\n",
      "ii=10700\n",
      "ii=10800\n",
      "ii=10900\n",
      "ii=11000\n",
      "ii=11100\n",
      "ii=11200\n",
      "ii=11300\n",
      "ii=11400\n",
      "ii=11500\n",
      "ii=11600\n",
      "ii=11700\n",
      "ii=11800\n",
      "ii=11900\n",
      "ii=12000\n",
      "ii=12100\n",
      "ii=12200\n",
      "ii=12300\n",
      "ii=12400\n",
      "ii=12500\n",
      "ii=12600\n",
      "ii=12700\n",
      "ii=12800\n",
      "ii=12900\n",
      "ii=13000\n",
      "ii=13100\n",
      "ii=13200\n",
      "ii=13300\n",
      "ii=13400\n",
      "ii=13500\n",
      "ii=13600\n",
      "ii=13700\n",
      "ii=13800\n",
      "ii=13900\n",
      "ii=14000\n",
      "ii=14100\n",
      "ii=14200\n",
      "ii=14300\n",
      "ii=14400\n",
      "ii=14500\n",
      "ii=14600\n",
      "ii=14700\n",
      "ii=14800\n",
      "ii=14900\n",
      "ii=15000\n",
      "ii=15100\n",
      "ii=15200\n",
      "ii=15300\n",
      "ii=15400\n",
      "ii=15500\n",
      "ii=15600\n",
      "ii=15700\n",
      "ii=15800\n",
      "ii=15900\n",
      "ii=16000\n",
      "ii=16100\n",
      "ii=16200\n",
      "ii=16300\n",
      "ii=16400\n",
      "ii=16500\n",
      "ii=16600\n",
      "ii=16700\n",
      "ii=16800\n",
      "ii=16900\n",
      "ii=17000\n",
      "ii=17100\n",
      "ii=17200\n",
      "ii=17300\n",
      "ii=17400\n",
      "ii=17500\n",
      "ii=17600\n",
      "ii=17700\n",
      "ii=17800\n",
      "ii=17900\n",
      "ii=18000\n",
      "ii=18100\n",
      "ii=18200\n",
      "ii=18300\n",
      "ii=18400\n",
      "ii=18500\n",
      "ii=18600\n",
      "ii=18700\n",
      "ii=18800\n",
      "ii=18900\n",
      "ii=19000\n",
      "ii=19100\n",
      "ii=19200\n",
      "ii=19300\n",
      "ii=19400\n",
      "ii=19500\n",
      "ii=19600\n",
      "ii=19700\n",
      "1660\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for ii in range(len(head_df)):\n",
    "    if((ii%100)==0):\n",
    "        print(f\"ii={ii}\")\n",
    "    if(head_df['SNTYPE'].iloc[ii]==0 or head_df['SNTYPE'].iloc[ii]==5 or head_df['SNTYPE'].iloc[ii]>100):\n",
    "        continue\n",
    "    if(head_df['REDSHIFT_HELIO'].iloc[ii]<0) or (head_df['FAKE'].iloc[ii]!=0):\n",
    "        continue\n",
    "   \n",
    "    data_1=[]\n",
    "    \n",
    "    row = head_df.iloc[ii]\n",
    "  \n",
    "    start = int(row['PTROBS_MIN'])\n",
    "    end = int(row['PTROBS_MAX']) + 1  # inclusive index\n",
    "\n",
    "    \n",
    "    table_attm1=[]\n",
    "    \n",
    "    countshow=countshow+1\n",
    "   \n",
    "    type=head_df['SNTYPE'].iloc[ii]\n",
    "    obj_id=head_df['SNID'].iloc[ii]\n",
    "    obj_id=int(obj_id.decode().strip())\n",
    "\n",
    "    redshift_p=head_df['REDSHIFT_HELIO'].iloc[ii]\n",
    "    #print(redshift_p)\n",
    "   \n",
    "    redshift_e=head_df['REDSHIFT_HELIO_ERR'].iloc[ii]\n",
    "    lc = phot_df.iloc[start:end].copy()\n",
    "\n",
    "    g=[]\n",
    "    r=[]\n",
    "    i=[]\n",
    "    z=[]\n",
    "    \n",
    "    gerr=[]\n",
    "    rerr=[] \n",
    "    ierr=[]\n",
    "    zerr=[]\n",
    "  \n",
    "    g_t=[]\n",
    "    r_t=[]\n",
    "    i_t=[]\n",
    "    z_t=[]\n",
    "  \n",
    "    count=0\n",
    "    fil=[]\n",
    "    array_mag=[]\n",
    "    array_time=[]\n",
    "    array_err=[]\n",
    "\n",
    "    lc['BAND'] = lc['BAND'].apply(lambda x: x.decode('utf-8').strip() if isinstance(x, bytes) else str(x).strip())\n",
    "   \n",
    "    # mag1=(-2.5*log10(lc['FLUXCAL'])+lc['ZEROPT']).to_numpy()\n",
    "\n",
    "    mag1=(-2.5*log10(lc['FLUXCAL'])+27.5).to_numpy()\n",
    "    mag_err1=abs(1.086*lc['FLUXCALERR']/lc['FLUXCAL']).to_numpy()\n",
    "    filter= lc['BAND'].to_numpy()\n",
    "    time=lc['MJD'].to_numpy()\n",
    "\n",
    "    mask=(~np.isnan(mag1)) & (mag_err1 < 0.5)\n",
    "    mag1 = mag1[mask]\n",
    "    mag_err1 = mag_err1[mask]\n",
    "    filter = filter[mask]\n",
    "    time = time[mask]\n",
    "\n",
    "    for jj in range(len(mag1)):\n",
    "         if(filter[jj]=='g'):\n",
    "            g.append(mag1[jj])\n",
    "            gerr.append(mag_err1[jj])\n",
    "            g_t.append(time[jj])\n",
    "\n",
    "         elif(filter[jj]=='r'):\n",
    "            r.append(mag1[jj])\n",
    "            rerr.append(mag_err1[jj])\n",
    "            r_t.append(time[jj])\n",
    "\n",
    "         elif(filter[jj]=='i'):\n",
    "            i.append(mag1[jj])\n",
    "            ierr.append(mag_err1[jj])\n",
    "            i_t.append(time[jj])\n",
    "\n",
    "         elif(filter[jj]=='z'):\n",
    "            z.append(mag1[jj])\n",
    "            zerr.append(mag_err1[jj])\n",
    "            z_t.append(time[jj])\n",
    "            \n",
    "        \n",
    "    if(g):\n",
    "        g=np.array(g)\n",
    "        if(len(g)>t1):\n",
    "            count=count+1\n",
    "            gerr=np.array(gerr)\n",
    "            g_t=np.array(g_t)\n",
    "            array_mag.append(g)\n",
    "            array_time.append(g_t)\n",
    "            array_err.append(gerr)\n",
    "            fil.append('g')\n",
    "        \n",
    "        \n",
    "\n",
    "    if(r):\n",
    "        r=np.array(r)\n",
    "        if(len(r)>t1):\n",
    "            count=count+1\n",
    "            rerr=np.array(rerr)\n",
    "            r_t=np.array(r_t) \n",
    "            array_mag.append(r)\n",
    "            array_time.append(r_t)\n",
    "            array_err.append(rerr) \n",
    "            fil.append('r')\n",
    "\n",
    "        \n",
    "    if(i):\n",
    "        i=np.array(i)\n",
    "        if(len(i)>t1):\n",
    "            count=count+1\n",
    "            ierr=np.array(ierr)\n",
    "            i_t=np.array(i_t) \n",
    "            array_mag.append(i)\n",
    "            array_time.append(i_t)\n",
    "            array_err.append(ierr)\n",
    "            fil.append('i')\n",
    "\n",
    "    if(z):\n",
    "        z=np.array(z)\n",
    "        if(len(i)>t1):\n",
    "            count=count+1\n",
    "            zerr=np.array(zerr)\n",
    "            z_t=np.array(z_t) \n",
    "            array_mag.append(z)\n",
    "            array_time.append(z_t)\n",
    "            array_err.append(zerr)\n",
    "            fil.append('z')\n",
    "\n",
    "    if(count>0):\n",
    "\n",
    "    \n",
    "        a_arr=np.zeros((len(array_mag)))\n",
    "        for kkk in range(len(array_mag)):\n",
    "            \n",
    "            a_arr[kkk]=len(array_mag[kkk])\n",
    "\n",
    "        b_arr=np.argsort(-a_arr)\n",
    "\n",
    "        fil = [fil[ipip] for ipip in b_arr]\n",
    "    \n",
    "        array_mag = [array_mag[ipip] for ipip in b_arr]\n",
    "        array_time = [array_time[ipip] for ipip in b_arr]\n",
    "\n",
    "        array_err = [array_err[ipip] for ipip in b_arr]\n",
    "        # print('count')\n",
    "        # print(count)\n",
    "        if((array_time[0][0]-head_df['PEAKMJD'].iloc[ii])<10):\n",
    "        \n",
    "\n",
    "            for momo in range(count):\n",
    "\n",
    "                    \n",
    "                if(fil[momo]=='g'):\n",
    "                    \n",
    "                    b1_now=redshift_p*redshift_p*mapping_coeff.loc['g', 'm1']+mapping_coeff.loc['g', 'c1']*redshift_p+mapping_coeff.loc['g', 'c0']\n",
    "                    b2_now=redshift_p*mapping_coeff.loc['g', 'm2']+mapping_coeff.loc['g', 'c2']\n",
    "                        \n",
    "                    sigma_b1_now=math.pow(mapping_coeff.loc['g', 'std_m1'],2)*math.pow(redshift_p,4)+math.pow(mapping_coeff.loc['g', 'std_c1'],2)*math.pow(redshift_p,2)+math.pow(mapping_coeff.loc['g', 'std_c0'],2)\n",
    "                    sigma_b2_now=square(mapping_coeff.loc['g', 'std_m2']*redshift_p)+square(mapping_coeff.loc['g', 'std_c2'])\n",
    "                    \n",
    "                elif(fil[momo]=='r'):\n",
    "                    b1_now=redshift_p*redshift_p*mapping_coeff.loc['r', 'm1']+mapping_coeff.loc['r', 'c1']*redshift_p+mapping_coeff.loc['r', 'c0']\n",
    "                    b2_now=redshift_p*mapping_coeff.loc['r', 'm2']+mapping_coeff.loc['r', 'c2']\n",
    "\n",
    "                    sigma_b1_now=math.pow(mapping_coeff.loc['r', 'std_m1'],2)*math.pow(redshift_p,4)+math.pow(mapping_coeff.loc['r', 'std_c1'],2)*math.pow(redshift_p,2)+math.pow(mapping_coeff.loc['r', 'std_c0'],2)\n",
    "                    sigma_b2_now=square(mapping_coeff.loc['r', 'std_m2']*redshift_p)+square(mapping_coeff.loc['r', 'std_c2'])\n",
    "\n",
    "\n",
    "                elif(fil[momo]=='i'):\n",
    "                    b1_now=redshift_p*mapping_coeff.loc['i', 'm1']+mapping_coeff.loc['i', 'c1']\n",
    "                    b2_now=redshift_p*mapping_coeff.loc['i', 'm2']+mapping_coeff.loc['i', 'c2'] \n",
    "\n",
    "\n",
    "                    sigma_b1_now=square(mapping_coeff.loc['i', 'std_m1']*redshift_p)+square(mapping_coeff.loc['i', 'std_c1'])\n",
    "                    sigma_b2_now=square(mapping_coeff.loc['i', 'std_m2']*redshift_p)+square(mapping_coeff.loc['i', 'std_c2'])\n",
    "                \n",
    "                elif(fil[momo]=='z'):\n",
    "                    b1_now=redshift_p*mapping_coeff.loc['z', 'm1']+mapping_coeff.loc['z', 'c1']\n",
    "                    b2_now=redshift_p*mapping_coeff.loc['z', 'm2']+mapping_coeff.loc['z', 'c2'] \n",
    "\n",
    "\n",
    "                    sigma_b1_now=square(mapping_coeff.loc['z', 'std_m1']*redshift_p)+square(mapping_coeff.loc['z', 'std_c1'])\n",
    "                    sigma_b2_now=square(mapping_coeff.loc['z', 'std_m2']*redshift_p)+square(mapping_coeff.loc['z', 'std_c2'])\n",
    "                \n",
    "                arr_pass=np.array([sqrt(sigma_b1_now), sqrt(sigma_b2_now)])\n",
    "                priors=np.array([b1_now, b2_now])\n",
    "                #print(array_mag[momo])\n",
    "                    \n",
    "                    \n",
    "                if(momo==0):\n",
    "                    #print(fil[momo])\n",
    "                    chi1=np.zeros(tr)\n",
    "                    final_params1=np.zeros((tr,4))\n",
    "                    final_errors1=np.zeros((tr,4))\n",
    "                    leng1=np.zeros(tr)\n",
    "\n",
    "                    final_params=np.zeros(4)\n",
    "                    final_errors=np.zeros(4)\n",
    "\n",
    "                    t_cons1=(array_time[0])[np.argmin(array_mag[0])]\n",
    "                    \n",
    "\n",
    "\n",
    "                    for mmmm in range(tr):\n",
    "                        # print('ini_params')\n",
    "                        # print(ini_params)\n",
    "                        ini_params=np.array([t_cons1+random.uniform(-3,+3)*(1+redshift_p), np.min(array_mag[momo]), b1_now+random.uniform(-2,2), b2_now+random.uniform(-2,2)])\n",
    "                        # print(fil[momo])\n",
    "                        # print(ini_params)\n",
    "                        final_params1[mmmm,:], final_errors1[mmmm,:], chi1[mmmm], leng1[mmmm] = mpfit_run_flux1(array_time[momo], array_mag[momo], array_err[momo], ini_params, t_cons1-10*(1+redshift_p), t_cons1+10 *(1+redshift_p), arr_pass, priors, redshift_p)\n",
    "                    \n",
    "                    \n",
    "                    # print('chi1')\n",
    "                    # print(chi1)\n",
    "                    min_ind=np.argmin(chi1)\n",
    "                    final_params[:]=final_params1[min_ind,:]\n",
    "                    final_errors[:]=final_errors1[min_ind,:]\n",
    "                    chi=chi1[min_ind]\n",
    "                    leng=leng1[min_ind]\n",
    "\n",
    " \n",
    "                    if(chi<10000):\n",
    "                        #print(redshift_p)\n",
    "                        t_cons2=final_params[0]\n",
    "                        \n",
    "                        data_1.append([final_params[0], final_params[1], final_params[2], final_params[3],\n",
    "                                        final_errors[0], final_errors[1], final_errors[2], final_errors[3], \n",
    "                                        chi, fil[momo],   redshift_p, redshift_e, leng, obj_id, ii, type])\n",
    "                        \n",
    "                        data_3.append([final_params[0], final_params[1], final_params[2], final_params[3],\n",
    "                                        final_errors[0], final_errors[1], final_errors[2], final_errors[3], \n",
    "                                        chi, fil[momo],   redshift_p, redshift_e, leng,  obj_id, ii, type])\n",
    "\n",
    "                    \n",
    "                        \n",
    "                        tmp_table = Table([array_time[momo], array_mag[momo], array_err[momo]],\n",
    "                                        names=('time', 'mag', 'magerr'),\n",
    "                                        meta={'redshift_p': redshift_p,\n",
    "                                            'redshift_e': redshift_e, 'filter':fil[momo]})\n",
    "                        tmp_table.write(\"tables/\"+str(ii)+'_'+fil[momo]+'.csv', overwrite=True)\n",
    "                        table_attm1.append(tmp_table)\n",
    "                        table_attm3.append(tmp_table)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    chi1=np.zeros(tr)\n",
    "                    final_params1=np.zeros((tr,4))\n",
    "                    final_errors1=np.zeros((tr,4))\n",
    "                    leng1=np.zeros(tr)\n",
    "\n",
    "                    final_params=np.zeros(4)\n",
    "                    final_errors=np.zeros(4)\n",
    "\n",
    "                   \n",
    "                    for mmmm in range(tr):\n",
    "                        # print('ini_params')\n",
    "                        # print(ini_params)\n",
    "                        \n",
    "                        ini_params=np.array([t_cons2+random.uniform(-2,2)*(1+redshift_p), np.min(array_mag[momo]), b1_now+random.uniform(-2,2), b2_now+random.uniform(-2,2)])\n",
    "                        \n",
    "                    \n",
    "                        final_params1[mmmm,:], final_errors1[mmmm,:], chi1[mmmm], leng1[mmmm] = mpfit_run_flux1(array_time[momo], array_mag[momo], array_err[momo], ini_params, t_cons2-7*(1+redshift_p), t_cons2+7*(1+redshift_p), arr_pass, priors, redshift_p)\n",
    "                       \n",
    "                    # print('chi1')\n",
    "                    # print(chi1)\n",
    "                    min_ind=np.argmin(chi1)\n",
    "                    final_params[:]=final_params1[min_ind,:]\n",
    "                    final_errors[:]=final_errors1[min_ind,:]\n",
    "                    chi=chi1[min_ind]\n",
    "                    leng=leng1[min_ind]\n",
    "\n",
    "                \n",
    "\n",
    "                    if(chi<10000):\n",
    "                        \n",
    "                        data_1.append([final_params[0], final_params[1], final_params[2], final_params[3],\n",
    "                                        final_errors[0], final_errors[1], final_errors[2], final_errors[3], \n",
    "                                        chi, fil[momo],  redshift_p, redshift_e, leng, obj_id, ii, type])\n",
    "                        \n",
    "                        data_3.append([final_params[0], final_params[1], final_params[2], final_params[3],\n",
    "                                        final_errors[0], final_errors[1], final_errors[2], final_errors[3], \n",
    "                                        chi, fil[momo],  redshift_p, redshift_e, leng, obj_id, ii, type])\n",
    "                        \n",
    "                        tmp_table = Table([array_time[momo], array_mag[momo], array_err[momo]],\n",
    "                                        names=('time', 'mag', 'magerr'),\n",
    "                                        meta={ 'redshift': redshift_p,\n",
    "                                            'redshift_e': redshift_e,  'filter':fil[momo]})\n",
    "                        tmp_table.write(\"tables/\"+str(ii)+'_'+fil[momo]+'.csv', overwrite=True)\n",
    "                        table_attm1.append(tmp_table)\n",
    "                        table_attm3.append(tmp_table)\n",
    "\n",
    "            #print(data_1)\n",
    "            data_2=np.array(data_1)\n",
    "            df = pd.DataFrame(data_2)\n",
    "            df.to_csv(\"mpfit_temp.csv\")\n",
    "            model1=pd.read_csv(\"mpfit_temp.csv\")\n",
    "            model=np.array(model1)\n",
    "            model=np.delete(model, 0, axis=1)\n",
    "            #print(len(model))\n",
    "\n",
    "            legen=[]\n",
    "\n",
    "\n",
    "            if(len(model)>0 ):\n",
    "\n",
    "                for j in range(len(model)):\n",
    "                \n",
    "                    plo_data_x=eigen_val[:,0]*(1+model[j,10])+model[j,0]-model[0,0]\n",
    "                    plo_data_y=model[j,1] + eigen_val[:,1] + model[j,2]*eigen_val[:,2] + model[j,3]*eigen_val[:,3]+dict_number[model[j,9]]\n",
    "                    \n",
    "                    # print('now')\n",
    "                    # print(np.nanmin(plo_data_y[1:]))\n",
    "                    # print(np.nanmax(plo_data_y[1:]))\n",
    "                    \n",
    "                   \n",
    "                    plt.plot(plo_data_x, plo_data_y, color=dict_color[model[j,9]],   linestyle='solid', label='_nolegend_') \n",
    "                    \n",
    "                    \n",
    "                    plt.errorbar(table_attm1[j][0][:]-model[0,0],  table_attm1[j][1][:]+dict_number[model[j,9]], yerr=table_attm1[j][2][:], color=dict_color[model[j,9]], fmt='*', markersize=8, elinewidth=1.5, label=dict_legend[model[j,9]])\n",
    "                    \n",
    "                \n",
    "                    coor=250\n",
    "                    plt.annotate(dict_legend[model[j,9]], xy = (plo_data_x[coor], plo_data_y[coor]), xytext = (1*plo_data_x[coor], 0.99*plo_data_y[coor]), color  = dict_color[model[j,9]])\n",
    "                    \n",
    "                    plo_data_y=np.array(plo_data_y)\n",
    "                    if(j==0):\n",
    "                        ymin=np.nanmin(plo_data_y[1:])\n",
    "                        ymax=np.nanmax(plo_data_y[1:])\n",
    "                    else:\n",
    "                        ymin=min(np.nanmin(plo_data_y[1:]), ymin)\n",
    "                        ymax=max(np.nanmax(plo_data_y[1:]), ymax)\n",
    "            \n",
    "                plt.xlim([-10*(1+model[0,10])-7, +40*(1+model[0,10])+7])\n",
    "                plt.ylim([ymin-0.5, ymax+0.5])\n",
    "                \n",
    "            \n",
    "                plt.xlabel('Time (Days)')\n",
    "                plt.ylabel('Magnitude')\n",
    "                plt.gca().invert_yaxis()\n",
    "\n",
    "    \n",
    "        \n",
    "                rw=np.round(model[j,10],2)\n",
    "               \n",
    "                plt.title('ID= '+str(obj_id)+'  '   + '  '      +'z= '+ str(rw) +'  '  +  '  type= '+ str(type) )\n",
    "                   \n",
    "                \n",
    "                plt.savefig(\"plots/\"+str(model[0,-2]) +'.png')\n",
    "                plt.close()\n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_4=np.array(data_3)\n",
    "df = pd.DataFrame(data_4)\n",
    "df.to_csv(\"des_good1.csv\", header=['pk_time', 'pk_mag', 'alpha', 'beta', 'err_time', 'err_mag',\n",
    "                                      'err_alpha', 'err_beta', 'chi', 'filter', \n",
    "                                     'photo_z', 'z_err', 'dof', 'obj_id', 'meta_id',  'type'], index=False)\n",
    "\n",
    "\n",
    "df=pd.read_csv('des_good1.csv')\n",
    "\n",
    "\n",
    "print(len(data_4))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b317cceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Successfully created the wide-format file: des_good_coverage.csv\n",
      "Final DataFrame has 456 rows (one per object).\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_long = pd.read_csv('des_good1.csv')\n",
    "\n",
    "\n",
    "\n",
    "df_long['meta_id'] = df_long['meta_id'].astype(int)\n",
    "df_long['filter'] = df_long['filter'].astype(str)\n",
    "\n",
    "# --- 2. Define Columns for Pivoting and Metadata ---\n",
    "\n",
    "# 8 columns per filter to be spread out\n",
    "value_cols = [\n",
    "    'pk_time', 'pk_mag', 'alpha', 'beta',\n",
    "    'err_time', 'err_mag', 'err_alpha', 'err_beta',\n",
    "]\n",
    "\n",
    "# Columns to keep as object metadata (should be consistent across all filters for an object)\n",
    "meta_cols = [\n",
    "    'photo_z', 'z_err', 'obj_id', 'type'\n",
    "]\n",
    "\n",
    "# --- 3. Pivot the Data from Long to Wide Format ---\n",
    "\n",
    "# Pivot on 'meta_id' to get one row per object, using 'filter' to spread the columns\n",
    "pivoted_df = df_long.pivot(\n",
    "    index='meta_id',\n",
    "    columns='filter',\n",
    "    values=value_cols\n",
    ")\n",
    "\n",
    "# --- 4. Flatten Multi-Index Columns and Rename ---\n",
    "\n",
    "# Flatten the column names to the format 'filter_parameter' (e.g., 'g_alpha')\n",
    "# The structure is (parameter_name, filter_name), so we reverse the order\n",
    "pivoted_df.columns = [f'{col[1]}_{col[0]}' for col in pivoted_df.columns]\n",
    "pivoted_df = pivoted_df.reset_index() # Move 'meta_id' from index back to a column\n",
    "\n",
    "# --- 5. Merge Metadata and Final Output ---\n",
    "\n",
    "# Extract unique metadata rows and merge them back with the pivoted data\n",
    "meta_unique_df = df_long[['meta_id'] + meta_cols].drop_duplicates(subset=['meta_id']).reset_index(drop=True)\n",
    "\n",
    "# Merge the metadata onto the wide-format results\n",
    "final_wide_df = meta_unique_df.merge(pivoted_df, on='meta_id', how='right')\n",
    "\n",
    "# 6. Save the final wide-format CSV file\n",
    "output_filename = 'des_good_coverage.csv'\n",
    "final_wide_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ Successfully created the wide-format file: {output_filename}\")\n",
    "print(f\"Final DataFrame has {len(final_wide_df)} rows (one per object).\")\n",
    "\n",
    "print(len(final_wide_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8648d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
